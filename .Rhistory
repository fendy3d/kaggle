<<<<<<< HEAD
CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2+
CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3, train_train)
train_train$CC1
class(train_train$CC1)
svm1 <- svm(train_train$Choice ~ CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1+
CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2+
CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3, train_train)
?svm
svm1 <- svm(train_train$Choice ~ CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1+
CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2+
CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3+
CC4+GN4+NS4+BU4+FA4+LD4+BZ4+FC4+FP4+RP4+PP4+KA4+SC4+TS4+NV4+MA4+LB4+AF4+HU4+Price4, data=train_train)
install.packages('neuralnet')
library("neuralnet")
traininginput <-  as.data.frame(runif(50, min=0, max=100))
trainingoutput <- sqrt(traininginput)
trainingdata <- cbind(traininginput,trainingoutput)
colnames(trainingdata) <- c("Input","Output")
head(trainingdata)
plot(trainingdata)
net.sqrt <- neuralnet(Output~Input,trainingdata, hidden=10, threshold=0.01)
print(net.sqrt)
plot(net.sqrt)
testdata <- as.data.frame((1:10)^2)
net.results <- compute(net.sqrt, testdata)
ls(net.results)
print(net.results$net.result)
cleanoutput <- cbind(testdata,sqrt(testdata),
as.data.frame(net.results$net.result))
colnames(cleanoutput) <- c("Input","Expected Output","Neural Net Output")
print(cleanoutput)
head(train)
unique(train$CC4)
unique(train$HU34)
unique(train$HU3)
unique(train$AF3)
unique(train$BZ4)
svm1 <- svm(Choice ~ CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1+
CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2+
CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3+
CC4+GN4+NS4+BU4+FA4+LD4+BZ4+FC4+FP4+RP4+PP4+KA4+SC4+TS4+NV4+MA4+LB4+AF4+HU4+Price4, data=train_train)
svm1 <- svm(Choice ~ CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1+
CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2+
CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3, data=train_train)
head(train_train,1)
?neuralnet
nn <- neuralnet(Choice~ CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1+
CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2+
CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3+
CC4+GN4+NS4+BU4+FA4+LD4+BZ4+FC4+FP4+RP4+PP4+KA4+SC4+TS4+NV4+MA4+LB4+AF4+HU4+Price4,data=train_train)
grep("CC1", colnames(train_train))
grep("Price4", colnames(train_train))
head(train_train[,4:83])
head(as.numeric(train_train[,4:83]))
class(train_train[,4:83])
class(train_train[,4])
class(train_train[,4])
class(as.numeric(train_train[,4]))
for (i in 4:83){
train_train[,i]<-as.numeric(train_train[,i])
}
class(train_train[,4])
nn <- neuralnet(Choice~ CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1+
CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2+
CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3+
CC4+GN4+NS4+BU4+FA4+LD4+BZ4+FC4+FP4+RP4+PP4+KA4+SC4+TS4+NV4+MA4+LB4+AF4+HU4+Price4,data=train_train)
for (i in 4:83){
train_train[,i]<-as.factor(train_train[,i])
}
class(train_train[,4])
nn <- neuralnet(Choice~ CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1+
nn <- neuralnet(Choice~ CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1+
CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2+
CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3+
CC4+GN4+NS4+BU4+FA4+LD4+BZ4+FC4+FP4+RP4+PP4+KA4+SC4+TS4+NV4+MA4+LB4+AF4+HU4+Price4,data=train_train)
train <- read.csv("train.csv", stringsAsFactors = FALSE)
train <- convertincometointeger(convertincome_train(train)) # converts '$290,000 to $299,999' to '$300,000 and over', then take the medium value.
train <- factorise(train) # factorise the variables (segment, year .. ppark)
set.seed(100)
library(caTools)
spl <- sample.split(train$Choice, SplitRatio = 0.7)
train_train <- subset(train, spl == TRUE)
train_test <- subset(train, spl == FALSE)
str(traindata)
str(train_train)
?model.matrix
m <- model.matrix(
~ Choice + CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1+
CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2+
CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3+
CC4+GN4+NS4+BU4+FA4+LD4+BZ4+FC4+FP4+RP4+PP4+KA4+SC4+TS4+NV4+MA4+LB4+AF4+HU4+Price4,
data = train_train
)
head(m)
tail(m)
unique(train_train$Choice)
nn <- neuralnet(Choice~ CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1+
CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2+
CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3+
CC4+GN4+NS4+BU4+FA4+LD4+BZ4+FC4+FP4+RP4+PP4+KA4+SC4+TS4+NV4+MA4+LB4+AF4+HU4+Price4,data=m)
dd <- data.frame(a = gl(3,4), b = gl(4,1,12))
dd
?gl
?model.matrix
model.matrix(~ a + b, dd)
m
head(m)
m <- model.matrix(
~ CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1+
CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2+
CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3+
CC4+GN4+NS4+BU4+FA4+LD4+BZ4+FC4+FP4+RP4+PP4+KA4+SC4+TS4+NV4+MA4+LB4+AF4+HU4+Price4,
data = train_train
)
m
head(m)
?cbind
head(train_train$Choice)
m <- cbind(m,train_train$Choice)
head(m)
m <- model.matrix(
~ CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1+
CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2+
CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3+
CC4+GN4+NS4+BU4+FA4+LD4+BZ4+FC4+FP4+RP4+PP4+KA4+SC4+TS4+NV4+MA4+LB4+AF4+HU4+Price4,
data = train_train
)
ncol(m)
ncol(train_train$Choice)
length(train_train$Choice)
nrow(train_train$Choice)
nrow(m)
class(m)
head(m)
class(m$CC1)
class(m[,1])
class(m[,2])
class(m[1,1])
m[1,1]
class(train_train$Choice)
??multinom
?multinom
head(traindata)
train <- read.csv("train.csv", stringsAsFactors = FALSE)
train <- convertincometointeger(convertincome_train(train)) # converts '$290,000 to $299,999' to '$300,000 and over', then take the medium value.
train <- factorise(train) # factorise the variables (segment, year .. ppark)
spl <- sample.split(train$Choice, SplitRatio = 0.7)
train_train <- subset(train, spl == TRUE)
train_test <- subset(train, spl == FALSE)
spl <- sample.split(train$Choice, SplitRatio = 0.6)
train_train <- subset(train, spl == TRUE)
train_test <- subset(train, spl == FALSE)
traindata <- mlogit.data(train, shape = "wide", choice = "Choice", varying = c(4:83), sep = "", alt.levels = c("Ch1","Ch2","Ch3","Ch4"), id.var = "Case")
train_traindata <- mlogit.data(train_train, shape = "wide", choice = "Choice", varying = c(4:83), sep = "", alt.levels = c("Ch1","Ch2","Ch3","Ch4"), id.var = "Case")
train_testdata <- mlogit.data(train_test, shape = "wide", choice = "Choice", varying = c(4:83), sep = "", alt.levels = c("Ch1","Ch2","Ch3","Ch4"), id.var = "Case")
train <- read.csv("train.csv")
test <- read.csv("test.csv")
traindata <- mlogit.data(train, shape = "wide", choice = "Choice", varying = c(4:83), sep = "", alt.levels = c("Ch1","Ch2","Ch3","Ch4"), id.var = "Case")
set.seed(42)
require(caret)
model <- train(Species~., data=iris, method='nnet',
trControl=trainControl(method='cv'))
model
probs <- predict(model, type='prob')
head(probs)
head(iris)
?train
class(iris)
head(train)
class(train)
head(train[,4:83])
head(train)
head(train[,99])
head(cbind(train[,4:83],train[,99])
)
head(train)
head(cbind(train[,4:83],train[0,99])
)
head(cbind(train[,4:83],train[1,99]))
class(train[1,99])
class(train[,99])
head(cbind(train[,4:83],train[,99]))
class(train[,4:83])
class(train[,99])
as.data.frame(train[,99])
class(as.data.frame(train[,99]))
head(cbind(train[,4:83],as.data.frame(train[,99])))
train[99]
head(train[99])
head(cbind(train[,4:83],train[99]))
class(cbind(train[,4:83],train[99]))
trainNN <- (cbind(train[,4:83],train[99]))
head(trainNN)
set.seed(100)
modelNN <- train(Choice~., data=trainNN, method='nnet',
trControl=trainControl(method='cv'))
model
require(caret)
model <- train(Species~., data=iris, method='nnet',
trControl=trainControl(method='cv'))
nrow(iris)
ncol(iris)
head(iris)
set.seed(100)
modelNN <- train(Choice~., data=trainNN, method='nnet',
trControl=trainControl(method='cv'))
model
AND <- c(rep(0,7),1)
head(AND)
AND
OR <- c(0,rep(1,7))
OR
binary.data <- data.frame(expand.grid(c(0,1), c(0,1), c(0,1)), AND, OR)
binary.data
print(net <- neuralnet(AND+OR~Var1+Var2+Var3,  binary.data, hidden=0,
rep=10, err.fct="ce", linear.output=FALSE))
XOR <- c(0,1,1,0)
xor.data <- data.frame(expand.grid(c(0,1), c(0,1)), XOR)
print(net.xor <- neuralnet(XOR~Var1+Var2, xor.data, hidden=2, rep=5))
plot(net.xor, rep="best")
?neuralnet
net <- neuralnet(AND+OR~Var1+Var2+Var3,  binary.data, hidden=0, rep=10, err.fct="sse", linear.output=FALSE))
net <- neuralnet(AND+OR~Var1+Var2+Var3,  binary.data, hidden=0, rep=10, err.fct="sse", linear.output=FALSE)
net
str(trainNN)
net <- neuralnet(Choice~.,  trainNN, hidden=0, rep=10, err.fct="sse", linear.output=FALSE)
net <- neuralnet(Choice~CC1+GN1+NS1+BU1+LD1+BZ1+FC1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1,
CC2+GN2+NS2+BU2+LD2+BZ2+FC2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2,
CC3+GN3+NS3+BU3+LD3+BZ3+FC3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3,
CC4+GN4+NS4+BU4+LD4+BZ4+FC4+PP4+KA4+SC4+TS4+NV4+MA4+LB4+AF4+HU4+Price4,trainNN, hidden=0, rep=10, err.fct="sse", linear.output=FALSE)
net <- neuralnet(Choice~CC1+GN1+NS1+BU1+LD1+BZ1+FC1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1+
CC2+GN2+NS2+BU2+LD2+BZ2+FC2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2+
CC3+GN3+NS3+BU3+LD3+BZ3+FC3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3+
CC4+GN4+NS4+BU4+LD4+BZ4+FC4+PP4+KA4+SC4+TS4+NV4+MA4+LB4+AF4+HU4+Price4,trainNN, hidden=0, rep=10, err.fct="sse", linear.output=FALSE)
binary.data
class(binary.data)
binary.data[,1]
class(binary.data[,1])
class(trainNN[,1])
trainNN[1,1]
nrow(trainNN)
ncol(trainNN)
head(trainNN,1)
trainNN[1,81]
for (i in 1:nrow(trainNN)){
if (trainNN[i,81]) == "Ch1"){
trainNN[i,81] = 1
} else if (trainNN[i,81]) == "Ch2"){
trainNN[i,81] = 2
} else if (trainNN[i,81]) == "Ch3"){
trainNN[i,81] = 3
} else if (trainNN[i,81]) == "Ch4"){
trainNN[i,81] = 4
}
for (i in 1:nrow(trainNN)){
if (trainNN[i,81]) == "Ch1"){
trainNN[i,81] = 1
} else if (trainNN[i,81]) == "Ch2"){
trainNN[i,81] = 2
} else if (trainNN[i,81]) == "Ch3"){
trainNN[i,81] = 3
} else if (trainNN[i,81]) == "Ch4"){
trainNN[i,81] = 4
}
}
head(trainNN)
trainNN[1,81]=="Ch2"
for (i in 1:nrow(trainNN)){
if (trainNN[i,81]) == "Ch1") 1
else if (trainNN[i,81]) == "Ch2") 2
else if (trainNN[i,81]) == "Ch3") 3
else 4
}
for (i in 1:nrow(trainNN)){
if (trainNN[i,81]) == "Ch1") {
trainNN[i,81]=1
} else if ( trainNN[i,81]) == "Ch2") {
trainNN[i,81]=2
} else if ( trainNN[i,81]) == "Ch3") {
trainNN[i,81]=3
} else
trainNN[i,81]=4
}
for (i in 1:nrow(trainNN)){
if (trainNN[i,81] == "Ch1") {
trainNN[i,81]=1
} else if ( trainNN[i,81] == "Ch2") {
trainNN[i,81]=2
} else if ( trainNN[i,81] == "Ch3") {
trainNN[i,81]=3
} else
trainNN[i,81]=4
}
trainNN
trainNN <- (cbind(train[,4:83],train[99]))
trainNN[81]
unique(trainNN[81])
unique(trainNN[81][1])
trainNN[81[1]]
trainNN[81][1]
trainNN[1,81]<-1
head(trainNN[81])
as.factor(trainNN[1,81])<-1
as.factor(trainNN[2,81])
class(trainNN[2,81])
class(trainNN[3,81])
head(trainNN)
trainNN$Choice[trainNN$Choice==Ch1]
trainNN$Choice[trainNN$Choice=="Ch1"]
trainNN$Choice[trainNN$Choice=="Ch1"]<-1
trainNN <- (cbind(train[,4:83],train[99]))
trainNN <- as.character(trainNN)
trainNN <- (cbind(train[,4:83],train[99]))
trainNN$Choice <- as.character(trainNN$Choice)
head(trainNN)
trainNN$Choice[trainNN$Choice=="Ch1"]<-1
trainNN$Choice
trainNN$Choice[trainNN$Choice=="Ch2"]<-2
trainNN$Choice[trainNN$Choice=="Ch3"]<-3
trainNN$Choice[trainNN$Choice=="Ch4"]<-4
trainNN$Choice
class(trainNN$Choice)
trainNN$Choice <- as.integer(trainNN$Choice)
class(trainNN$Choice)
set.seed(100)
net <- neuralnet(Choice~CC1+GN1+NS1+BU1+LD1+BZ1+FC1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1+
CC2+GN2+NS2+BU2+LD2+BZ2+FC2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2+
CC3+GN3+NS3+BU3+LD3+BZ3+FC3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3+
CC4+GN4+NS4+BU4+LD4+BZ4+FC4+PP4+KA4+SC4+TS4+NV4+MA4+LB4+AF4+HU4+Price4,trainNN, hidden=0, rep=10, err.fct="sse", linear.output=FALSE)
net
?compute
class(net)
Var1 <- runif(50, 0, 100)
sqrt.data <- data.frame(Var1, Sqrt=sqrt(Var1))
sqrt.data
print(net.sqrt <- neuralnet(Sqrt~Var1, sqrt.data, hidden=10,
)
print(net.sqrt <- neuralnet(Sqrt~Var1, sqrt.data, hidden=10,threshold=0.01))
net.sqrt
compute(net.sqrt, (1:10)^2)$net.result
net.result
(1:10)^2
sqrt.data
compute(net.sqrt, (1:10)^2)$net.result
(1:10)^2
net
trainNN <- (cbind(train[,4:83],train[99]))
set.seed(100)
head(trainNN)
trainNN$Choice <- as.character(trainNN$Choice)
model <- train(Choice~., data=trainNN, method='nnet',trControl=trainControl(method='cv'))
require(caret)
model <- train(Species~., data=iris, method='nnet',
trControl=trainControl(method='cv'))
model
probs <- predict(model, type='prob')
head(probs)
model <- train(Choice~., data=trainNN, method='nnet',trControl=trainControl(method='cv'))
model
trainNN
trainNN$Choice
class(trainNN$Choice)
trainNN <- (cbind(train[,4:83],train[99]))
set.seed(100)
model <- train(Choice~., data=trainNN, method='nnet',trControl=trainControl(method='cv'))
model
probs <- predict(model, type='prob')
head(probs)
testNN <- test[,4:83]
compute(model, testNN)$net.result
testNN
class(testNN)
ncol(testNN)
head(testNN,1)
nrow(testNN,1)
nrow(testNN)
nrow(na.omit(testNN))
compute(model, testNN)$model.result
class(model)
probs <- predict(model, newdata=testNN, type='prob')
head(probs)
nrow(testNN)
nrow(probs)
nrow(trainNN)
write.table(probs,file="nn.csv",row.names=FALSE)
write.table(probs,file="nn.csv",sep=",",row.names=FALSE)
head(train)
head(train_train)
train_trainNN <- (cbind(train_train[,4:83],train_train[99]))
train_testNN <- (cbind(train_test[,4:83],train_test[99]))
train_testNN <- (cbind(train_test[,4:83]))
train_testNN
head(train_testNN)
model_traintrain <- train(Choice~., data=train_trainNN, method='nnet',trControl=trainControl(method='cv'))
class(train_trainNN$Choice)
train <- read.csv("train.csv", stringsAsFactors = FALSE)
train <- convertincometointeger(convertincome_train(train)) # converts '$290,000 to $299,999' to '$300,000 and over', then take the middle value.
train <- factorise(train) # factorise the variables (segment, year .. ppark)
set.seed(100)
library(caTools)
spl <- sample.split(train$Choice, SplitRatio = 0.7)
train_train <- subset(train, spl == TRUE)
train_test <- subset(train, spl == FALSE)
train_trainNN <- (cbind(train_train[,4:83],train_train[99]))
train_testNN <- (cbind(train_test[,4:83]))
class(train_trainNN$Choice)
train_trainNN$Choice <- as.factor(train_trainNN$Choice)
model_traintrain <- train(Choice~., data=train_trainNN, method='nnet',trControl=trainControl(method='cv'))
compute(model_traintrain, train_testNN)model_traintrain.result
compute(model_traintrain, train_testNN)$model_traintrain.result
probs2 <- predict(model_traintrain, newdata=train_testNN, type='prob')
head(probs2)
probs <- predict(model, newdata=testNN, type='prob')
probs1 <- predict(model, newdata=testNN, type='prob')
head(probs1)
0.25120472322 +0.1535741385 +0.16231621215+ 0.4329049261
write.table(probs,file="nn.csv",sep=",",row.names=FALSE)
head(probs2)
head(train_test$Choice)
?nnet
?trcontrol
?train
?nnet
ir <- rbind(iris3[,,1],iris3[,,2],iris3[,,3])
ir
head(ir)
iris3
head(iris3)
head(iris2)
head(iris1)
class(iris3)
iris3[,,1]
head(ir)
head(iris3[,,1])
head(iris3[,,2])
head(ir)
ir
ir <- rbind(iris3[,,1],iris3[,,2],iris3[,,3])
ir
targets <- class.ind( c(rep("s", 50), rep("c", 50), rep("v", 50)) )
samp <- c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
head(samp)
samp
targets
iris3
targets
iris3
sample(1:50,25)
sample(1:50,25)
set.seed(100)
samp <- c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
samp
sample(1:50,25)
samp
sample(1:50,25)
set.seed(100)
samp <- c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
sample(1:50,25)
samp
ir[samp,]
targets[samp,]
ir1 <- nnet(ir[samp,], targets[samp,], size = 2, rang = 0.1, decay = 5e-4, maxit = 200)
test.cl <- function(true, pred) {
true <- max.col(true)
cres <- max.col(pred)
table(true, cres)
}
targets[-samp,]
targets[samp,]
targets[-samp,]
ir[samp,]
train_train[,4:83]
head(train_train[,4:83],1)
length(head(train_train[,4:83],1))
nnmodel1 <- nnet(train_train[,4:83],train_train[99],size = 38, rang = 0.1, decay = 5e-4, maxit = 200 )
head(train_train[1])
head(train_train,1)
nnmodel1 <- nnet(Choice~CC1+GN1+NS1+BU1+LD1+BZ1+FC1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1+
CC2+GN2+NS2+BU2+LD2+BZ2+FC2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2+
CC3+GN3+NS3+BU3+LD3+BZ3+FC3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3+
CC4+GN4+NS4+BU4+LD4+BZ4+FC4+PP4+KA4+SC4+TS4+NV4+MA4+LB4+AF4+HU4+Price4,train_train,size = 38, rang = 0.1, decay = 5e-4, maxit = 200 )
nnmodel1 <- nnet(Choice~CC1+GN1+NS1+BU1+LD1+BZ1+FC1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1+
CC2+GN2+NS2+BU2+LD2+BZ2+FC2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2+
CC3+GN3+NS3+BU3+LD3+BZ3+FC3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3+
CC4+GN4+NS4+BU4+LD4+BZ4+FC4+PP4+KA4+SC4+TS4+NV4+MA4+LB4+AF4+HU4+Price4,train_train,size = 2, rang = 0.1, decay = 5e-4, maxit = 200 )
nnmodel1 <- nnet(train_train[,4:83],train_train[99],size = 10, rang = 0.1, decay = 5e-4, maxit = 200 )
nnmodel1
nnmodel1 <- nnet(train_train[,4:83],train_train[99],size = 2, rang = 0.1, decay = 5e-4, maxit = 200 )
nrow(train_train)
nrow(na.omit(train_train))
nnmodel1 <- nnet(train_train[,4:83],train_train[99],size = 2, rang = 0.1, decay = 5e-4, maxit = 200 )
traceback()
class(train_train[90])
head(train_train[90])
head(train_train[99])
class(head(train_train[99]))
class(head(train_train[99][1]))
train_train[99][1]
train_train[,99]
class(train_train[,99])
train_train[99]
class(train_train[99])
train_train[,99]
class(train_train[,99])
head(train_train[,99])
class(train_train[,4:83])
nnmodel1 <- nnet(train_train[,4:83],train_train[,99],size = 2, rang = 0.1, decay = 5e-4, maxit = 200 )
nnmodel1
nnmodel1 <- nnet(train_train[,4:83],train_train[,99],size = 2, rang = 0.1, decay = 5e-4, maxit = 200 )
nnmodel1 <- nnet(train_train[,4:83],train_train[,99],size = 38, rang = 0.1, decay = 5e-4, maxit = 200 )
class(nnmodel1)
1+1
=======
summary(model1)
predtest1<-predict(model1, newdata=test)
sse1<-sum((test$Apps-predtest1)^2)
sse1
sse1<-mean((test$Apps-predtest1)^2)
sse1
mse
mse<-mean((test$Apps-predtest1)^2)
mse
ncol(College)
str(college)
str(College)
library(leaps)
model2.fwd <- regsubsets(Apps ~ ., data = College, nvmax = ncol(College) - 1, method = "forward")
summary(model2.fwd)
ncol(Hitters)
val.errors<-rep(NA,17)
for(i in 1:17){
coefi<-coef(model2.fwd,id=i)
pred<- test.mat[,names(coefi)]%*%coefi   # there is no predict() method for regsubsets()
val.errors[i]<-mean((test$Apps-pred)^2)
}
val.errors
test.mat<-model.matrix(Apps ~ ., data=test)
val.errors<-rep(NA,17)
for(i in 1:17){
coefi<-coef(model2.fwd,id=i)
pred<- test.mat[,names(coefi)]%*%coefi   # there is no predict() method for regsubsets()
val.errors[i]<-mean((test$Apps-pred)^2)
}
val.errors
which.min(val.errors)
var.length <- ncol(College) - 1
test.mat<-model.matrix(Apps ~ ., data=test)
var.length <- ncol(College) - 1
var.length
k<-10
set.seed(1)
folds <- sample(1:k, nrow(College), replace=TRUE)
cv.errors<-matrix(NA, nrow=k, ncol=var.length, dimnames=list(NULL, paste(1:var.length)))
cv.errors
predict.regsubsets <- function(object,newdata,id,...){
form <- as.formula(object$call[[2]])
mat <- model.matrix(form,newdata)
coefi <- coef(object,id=id)
return(mat[,names(coefi)]%*%coefi)
}
for (j in 1:k){
fwd.fit <- regsubsets(Apps ~. ,data = College[folds!=j,], nvmax = var.length, method = "Forward")
for (i in 1:var.length){
pred <- predict.regsubsets(fwd.fit, College[folds==j,], id = i)
cv.errors[j,i] <- mean((College$Apps[folds==j]-pred)^2)
}
}
library(leaps)
for (j in 1:k){
fwd.fit <- regsubsets(Apps ~. ,data = College[folds!=j,], nvmax = var.length, method = "forward")
for (i in 1:var.length){
pred <- predict.regsubsets(fwd.fit, College[folds==j,], id = i)
cv.errors[j,i] <- mean((College$Apps[folds==j]-pred)^2)
}
}
cv.errors
?apply
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors
mean(cv.errors[,1])
which.min(mean.cv.errors)
?"glmnet"
train.mat<-model.matrix(Apps ~ ., data = train)
var.length <- ncol(College) - 1
k<-10
set.seed(1)
folds <- sample(1:k, nrow(College), replace = TRUE)
cv.errors <- matrix(NA, nrow = k, ncol = var.length, dimnames = list(NULL, paste(1:var.length)))
cv.errors #creating an empty 10 x 17 matrix with ncol = 17 for each fold of the model
predict.regsubsets <- function(object,newdata,id,...){
form <- as.formula(object$call[[2]])
mat <- model.matrix(form,newdata)
coefi <- coef(object,id=id)
}
return(mat[,names(coefi)]%*%coefi)
for (j in 1:k){
fwd.fit <- regsubsets(Apps ~. ,data = College[folds!=j,], nvmax = var.length, method = "forward")
for (i in 1:var.length){
pred <- predict.regsubsets(fwd.fit, College[folds==j,], id = i)
cv.errors[j,i] <- mean((College$Apps[folds==j]-pred)^2)
}
}
cv.errors # This is the answer to the question
mean.cv.errors <- apply(cv.errors, 2, mean) #2 here means apply the mean on the data in a column
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors,type='b')
which.min(mean.cv.errors)
train.mat <- model.matrix(Apps ~ ., data = College.train)
train.mat <- model.matrix(Apps ~ ., data = train)
test.mat <- model.matrix(Apps ~ ., data = test)
grid<-10^seq(4,-2, length=100)
grid
ridge.model <- glmnet(train.mat, train$Apps, alpha=0, lambda=grid)
?"glmnet"
cv.ridge <- cv.glmnet(train.mat, train$Apps, alpha = 0, lambda = grid)
cv.ridge
bestlambda <- cv.ridge$lambda.min
bestlambda
?predict
pred.ridge <- predict(ridge.model, s = bestlambda, newx = test.mat)
mean((pred.ridge - test$Apps)^2)
ridge.model <- glmnet(train.mat, train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
cv.ridge <- cv.glmnet(train.mat, train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
bestlambda <- cv.ridge$lambda.min
bestlambda
pred.ridge <- predict(ridge.model, s = bestlambda, newx = test.mat)
mean((pred.ridge - test$Apps)^2)
test <- -t
test <- -t
train2 <- College[test,]
train2
testt <- College[!t,]
testt
t <- sample(c(T,F), nrow(College), replace = TRUE) #sample(x, size, replace = TRUE) x: a vector of elements from which to sample, size: is the number of times to sample, replace: replace elements after sampling
train <- College[t,]
test <- College[!t,]
model1 <- lm(Apps ~ ., data = train)
predtest1<-predict(model1, newdata=test)
mse<-mean((test$Apps-predtest1)^2)
library(leaps)
train.mat <- model.matrix(Apps ~ ., data = train)
var.length <- ncol(College) - 1
k<-10
set.seed(1)
folds <- sample(1:k, nrow(College), replace = TRUE)
cv.errors <- matrix(NA, nrow = k, ncol = var.length, dimnames = list(NULL, paste(1:var.length)))
cv.errors #creating an empty 10 x 17 matrix with ncol = 17 for each fold of the model
predict.regsubsets <- function(object,newdata,id,...){
form <- as.formula(object$call[[2]])
mat <- model.matrix(form,newdata)
coefi <- coef(object,id=id)
return(mat[,names(coefi)]%*%coefi)
}
for (j in 1:k){
fwd.fit <- regsubsets(Apps ~. ,data = College[folds!=j,], nvmax = var.length, method = "forward")
for (i in 1:var.length){
pred <- predict.regsubsets(fwd.fit, College[folds==j,], id = i)
cv.errors[j,i] <- mean((College$Apps[folds==j]-pred)^2)
}
}
cv.errors # This is the answer to the question
mean.cv.errors <- apply(cv.errors, 2, mean) #2 here means apply the mean on the data in a column
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors,type='b')
which.min(mean.cv.errors) #gives 1
train.mat <- model.matrix(Apps ~ ., data = train) # creating the "x" with the training data set
test.mat <- model.matrix(Apps ~ ., data = test) # creating the "x" with the test data set
grid <- 10^seq(4,-2, length=100) # creating a lot of lambda values so that with each value, it changes the coefficients of the model
ridge.model <- glmnet(train.mat, train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
cv.ridge <- cv.glmnet(train.mat, train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
bestlambda <- cv.ridge$lambda.min
bestlambda
pred.ridge <- predict(ridge.model, s = bestlambda, newx = test.mat)
mean((pred.ridge - test$Apps)^2)
set.seed(1)
x
nrow(x)
dim(College)
nrow(train.mat)
x<-model.matrix(Apps ~ ., data = College)[,-1]
nrow(x)
College.train <- College[train, ]
College.test <- College[test, ]
train <- sample(1:dim(College)[1], dim(College)[1]/2)
test<- -train
College.train <- College[train, ]
College.test <- College[test, ]
train.mat <- model.matrix(Apps ~ ., data = College.train) # creating the "x" with the training data set
test.mat <- model.matrix(Apps ~ ., data = College.test) # creating the "x" with the test data set
grid <- 10^seq(4,-2, length=100) # creating a lot of lambda values so that with each value, it changes the coefficients of the model
ridge.model <- glmnet(train.mat, train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
cv.ridge <- cv.glmnet(train.mat, train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
ridge.model <- glmnet(train.mat, College.train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
cv.ridge <- cv.glmnet(train.mat, College.train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
bestlambda <- cv.ridge$lambda.min
bestlambda
pred.ridge <- predict(ridge.model, s = bestlambda, newx = test.mat)
mean((pred.ridge - test$Apps)^2)
mean((pred.ridge - College.test$Apps)^2)
set.seed(11)
train <- sample(1:dim(College)[1], dim(College)[1]/2)
test<- -train
College.train <- College[train, ]
College.test <- College[test, ]
train.mat <- model.matrix(Apps ~ ., data = College.train) # creating the "x" with the training data set
test.mat <- model.matrix(Apps ~ ., data = College.test) # creating the "x" with the test data set
grid <- 10^seq(4,-2, length=100) # creating a lot of lambda values so that with each value, it changes the coefficients of the model
ridge.model <- glmnet(train.mat, College.train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
cv.ridge <- cv.glmnet(train.mat, College.train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
bestlambda <- cv.ridge$lambda.min
bestlambda
pred.ridge <- predict(ridge.model, s = bestlambda, newx = test.mat)
mean((pred.ridge - College.test$Apps)^2)
set.seed(1)
train <- sample(1:dim(College)[1], dim(College)[1]/2)
test<- -train
College.train <- College[train, ]
College.test <- College[test, ]
train.mat <- model.matrix(Apps ~ ., data = College.train) # creating the "x" with the training data set
test.mat <- model.matrix(Apps ~ ., data = College.test) # creating the "x" with the test data set
grid <- 10^seq(4,-2, length=100) # creating a lot of lambda values so that with each value, it changes the coefficients of the model
ridge.model <- glmnet(train.mat, College.train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
cv.ridge <- cv.glmnet(train.mat, College.train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
bestlambda
bestlambda <- cv.ridge$lambda.min
pred.ridge <- predict(ridge.model, s = bestlambda, newx = test.mat)
mean((pred.ridge - College.test$Apps)^2)
library(glmnet)
set.seed(11)
train <- sample(1:dim(College)[1], dim(College)[1]/2)
test<- -train
College.train <- College[train, ]
College.test <- College[test, ]
train.mat <- model.matrix(Apps ~ ., data = College.train) # creating the "x" with the training data set
test.mat <- model.matrix(Apps ~ ., data = College.test) # creating the "x" with the test data set
grid <- 10^seq(4,-2, length=100) # creating a lot of lambda values so that with each value, it changes the coefficients of the model
lasso.model <- glmnet(train.mat, College.train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
cv.lasso <- cv.glmnet(train.mat, College.train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
bestlambda <- cv.lasso$lambda.min
bestlambda
pred.lasso <- predict(lasso.model, s = bestlambda, newx = test.mat)
mean((pred.lasso - College.test$Apps)^2)
lasso.model <- glmnet(train.mat, College.train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
cv.lasso <- cv.glmnet(train.mat, College.train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
bestlambda <- cv.lasso$lambda.min
bestlambda
pred.lasso <- predict(lasso.model, s = bestlambda, newx = test.mat)
mean((pred.lasso - College.test$Apps)^2)
```
0.7*0.7*0.6
svm1 <- svm(Class ~ ., data = traindatanew) # "cross" 10 cross-validations
library(e1071)
svm1 <- svm(Class ~ ., data = traindatanew) # "cross" 10 cross-validations
convertincometointeger <- function(train){
train$income <- ifelse(train$income == "Under $29,999", 15000, ifelse(train$income == "$30,000 to $39,999", 35000, ifelse(train$income =="$40,000 to $49,999", 45000, ifelse(train$income =="$50,000 to $59,999", 55000, ifelse(train$income =="$60,000 to $69,999", 65000, ifelse(train$income =="$70,000 to $79,999", 75000, ifelse(train$income =="$80,000 to $89,999", 85000, ifelse(train$income =="$90,000 to $99,999", 95000, ifelse(train$income =="$100,000 to $109,999", 105000, ifelse(train$income == "$110,000 to $119,999", 115000, ifelse(train$income == "$120,000 to $129,999", 125000, ifelse(train$income == "$130,000 to $139,999", 135000, ifelse(train$income == "$140,000 to $149,999", 145000, ifelse(train$income == "$150,000 to $159,999", 155000, ifelse(train$income == "$160,000 to $169,999", 165000, ifelse(train$income == "$170,000 to $179,999", 175000, ifelse(train$income == "$190,000 to $199,999", 195000, ifelse(train$income == "$200,000 to $209,999", 205000, ifelse(train$income == "$220,000 to $229,999", 225000, ifelse(train$income == "$250,000 to $259,999", 255000, ifelse(train$income == "$300,000 & Over", 320000, NA)))))))))))))))))))))
return (train)
}
convertpricetointeger <- function(train){
train <- ifelse(train == 1, 500, ifelse(train == 2, 1000, ifelse(train ==3, 1500, ifelse(train == 4, 2000, ifelse(train == 5, 2500, ifelse(train == 6, 3000, ifelse(train == 7, 4000, ifelse(train == 8, 5000, ifelse(train == 9, 7500, ifelse(train == 10, 10000, ifelse(train == 11, 12000, 1)))))))))))
return (train)
}
# manage inconsistencies in income (only for training)
convertincome_train <- function(train){
N <- nrow(train)
for (i in 1:N){
if (train[i,93] == "$290,000 to $299,999"){
train[i,93] <- as.character("$300,000 & Over")
}
}
return (train)
}
# manage inconsistencies in income (only for test)
convertincome_test <- function(test){
N <- nrow(test)
for (i in 1:N){
if (as.character(test[i,93]) == "$180,000 to $189,999"){
test[i,93] = as.character("$170,000 to $179,999")
} else if (as.character(test[i,93]) == "$230,000 to $239,999"){
test[i,93] = as.character("$220,000 to $229,999")
} else if (as.character(test[i,93]) == "$270,000 to $279,999"){
test[i,93] = as.character("$250,000 to $259,999")
}
return (test)
}
}
# factorise the variables (segment, year .. ppark)
factorise <- function(train){
train$segment <- factor(train$segment)
train$year <- factor(train$year)
train$miles <- factor(train$miles)
train$night <- factor(train$night)
train$gender <- factor(train$gender)
train$age <- factor(train$age)
train$educ <- factor(train$educ)
train$region <- factor(train$region)
train$Urb <- factor(train$Urb)
train$ppark <- factor(train$ppark)
return (train)
}
train <- read.csv("train.csv", stringsAsFactors = FALSE)
train <- convertincometointeger(convertincome_train(train)) # converts '$290,000 to $299,999' to '$300,000 and over', then take the middle value.
train <- factorise(train) # factorise the variables (segment, year .. ppark)
train$Price1 <- convertpricetointeger(train$Price1)
train$Price2 <- convertpricetointeger(train$Price2)
train$Price3 <- convertpricetointeger(train$Price3)
train$Price4 <- convertpricetointeger(train$Price4)
setwd("C:/Users/Guanhua/Dropbox/SUTD/LECTURE NOTES/Term 8/Analytics wd/Kaggle/kaggle")
train <- read.csv("train.csv", stringsAsFactors = FALSE)
train <- convertincometointeger(convertincome_train(train)) # converts '$290,000 to $299,999' to '$300,000 and over', then take the middle value.
train <- factorise(train) # factorise the variables (segment, year .. ppark)
train$Price1 <- convertpricetointeger(train$Price1)
train$Price2 <- convertpricetointeger(train$Price2)
train$Price3 <- convertpricetointeger(train$Price3)
train$Price4 <- convertpricetointeger(train$Price4)
library(mlogit)
traindata <- mlogit.data(train, shape = "wide", choice = "Choice", varying = c(4:83), sep = "", alt.levels = c("Ch1","Ch2","Ch3","Ch4"), id.var = "Case")
View(traindata)
traindata$Class <- 0
for (i in 1:38000){
for (j in 15:18){
if (traindata[i,j] == 1){
traindata[i,]$Class <- colnames(traindata[j])
}
}
}
write.csv(traindata, "traindata.csv")
testdata$Class <- 0
test <- read.csv("test.csv", stringsAsFactors = FALSE)
test[is.na(test)] <- 0 # adds 0 to all missing values
test <- convertincometointeger(convertincome_test(test))
test <- factorise(test)
test$Price1 <- convertpricetointeger(test$Price1)
test$Price2 <- convertpricetointeger(test$Price2)
test$Price3 <- convertpricetointeger(test$Price3)
test$Price4 <- convertpricetointeger(test$Price4)
testdata <- mlogit.data(test, shape = "wide", choice = "Choice", varying = c(4:83), sep = "", alt.levels = c("Ch1","Ch2","Ch3","Ch4"), id.var = "Case")
testdata$Class <- 0
traindata <- read.csv("traindata.csv")
library(caTools)
train_train <- subset(traindata, spl == TRUE)
spl <- sample.split(traindata$Class, SplitRatio = 0.7)
train_train <- subset(traindata, spl == TRUE)
test_train <- subset(traindata, spl == FALSE)
View(train_train)
train_trainnew <- subset(train_train, select = - c(1,2,3,15,16,17,18,19,20,41)) # remove ch1,ch2,ch3,ch4,chid,alt columns
train_trainnew$Class <- as.factor(traindatanew$Class)
train_trainnew$Class <- as.factor(train_trainnew$Class)
testdata <- subset(test_rf, select = - c(1,2,3,15,16,17,18,19,20,41)) # remove ch1,ch2,ch3,ch4,chid,alt columns
train_test <- subset(traindata, spl == FALSE)
train_testnew <- subset(train_test, select = - c(1,2,3,15,16,17,18,19,20,41)) # remove ch1,ch2,ch3,ch4,chid,alt columns
svm1 <- svm(Class ~ ., data = train_trainnew) # "cross" 10 cross-validations
predsvm1 <- predict(svm1, newdata = train_testnew, type = "hhh")
head(predsvm1)
predsvm1 <- predict(svm1, newdata = train_testnew, type = "probability")
head(predsvm1)
?svm
summary(smv1)
svm1
summary(svm1)
svm1 <- svm(Class ~ ., data = train_trainnew, probability = TRUE)
predsvm1 <- predict(svm1, newdata = train_testnew)
head(predsvm1)
predsvm1 <- predict(svm1, newdata = train_testnew, probability = TRUE)
head(predsvm1)
predsvm1 <- predict(svm1, newdata = train_testnew, decision.values = TRUE, probability = TRUE)
head(predsvm1)
library(RWeka)
library(FSelector)
information.gain(Class ~ ., data = train_trainnew)
str(train_trainnew)
View(traindata)
traindata[,1]
traindata[,2]
names(traindata[,2])
colnames(traindata[,2])
traindata[41,]
traindata[,41]
train_trainnew <- subset(train_train, select = - c(2,3,4,16,17,18,19,20,21,42)) # remove ch1,ch2,ch3,ch4,chid,alt columns
spl <- sample.split(traindata$Class, SplitRatio = 0.7)
train_train <- subset(traindata, spl == TRUE)
train_test <- subset(traindata, spl == FALSE)
train_trainnew <- subset(train_train, select = - c(2,3,4,16,17,18,19,20,21,42)) # remove ch1,ch2,ch3,ch4,chid,alt columns
train_trainnew$Class <- as.factor(train_trainnew$Class)
train_testnew <- subset(train_test, select = - c(2,3,4,16,17,18,19,20,21,42)) # remove ch1,ch2,ch3,ch4,chid,alt columns
View(train_trainnew)
str(train_trainnew)
train_trainnew$year <- factor(train_trainnew)
spl <- sample.split(traindata$Class, SplitRatio = 0.7)
train_train <- subset(traindata, spl == TRUE)
train_test <- subset(traindata, spl == FALSE)
traindata <- read.csv("traindata.csv")
traindata$Class <- as.factor(train_trainnew$Class)
traindata <- read.csv("traindata.csv")
traindata$Class <- factor(traindata$Class)
traindata$year <- factor(traindata)
traindata$year <- factor(traindata$year)
library(caTools)
spl <- sample.split(traindata$Class, SplitRatio = 0.7)
train_train <- subset(traindata, spl == TRUE)
train_test <- subset(traindata, spl == FALSE)
train_trainnew <- subset(train_train, select = - c(2,3,4,16,17,18,19,20,21,42)) # remove ch1,ch2,ch3,ch4,chid,alt columns
train_testnew <- subset(train_test, select = - c(2,3,4,16,17,18,19,20,21,42)) # remove ch1,ch2,ch3,ch4,chid,alt columns
test <- read.csv("test.csv", stringsAsFactors = FALSE)
test[is.na(test)] <- 0 # adds 0 to all missing values
test <- convertincometointeger(convertincome_test(test))
test <- factorise(test)
test$Price1 <- convertpricetointeger(test$Price1)
test$Price2 <- convertpricetointeger(test$Price2)
test$Price3 <- convertpricetointeger(test$Price3)
test$Price4 <- convertpricetointeger(test$Price4)
testdata <- mlogit.data(test, shape = "wide", choice = "Choice", varying = c(4:83), sep = "", alt.levels = c("Ch1","Ch2","Ch3","Ch4"), id.var = "Case")
library(RWeka)
library(FSelector)
information.gain(Class ~ ., data = train_trainnew)
traindata <- read.csv("traindata.csv")
traindata$Class <- factor(traindata$Class)
traindata$year <- factor(traindata$year)
View(traindata)
library(caTools)
spl <- sample.split(traindata$Class, SplitRatio = 0.7)
train_train <- subset(traindata, spl == TRUE)
train_test <- subset(traindata, spl == FALSE)
train_trainnew <- subset(train_train, select = - c(1,2,3,4,16,17,18,19,20,21,42)) # remove ch1,ch2,ch3,ch4,chid,alt columns
train_testnew <- subset(train_test, select = - c(1,2,3,4,16,17,18,19,20,21,42)) # remove ch1,ch2,ch3,ch4,chid,alt columns
information.gain(Class ~ ., data = train_trainnew)
source('~/.active-rstudio-document', echo=TRUE)
information.gain(Class ~ ., data = train_trainnew)
modelC45 <- J48(Class ~ ., data = train_trainnew)
WOW(J48)
modelC45 <- J48(Class ~ ., data = train_trainnew, control = Weka_control(A = TRUE))
predC45 <- predict(modelC45, newdata = train_testnew, type = "haha")
predC45 <- predict(modelC45, newdata = train_testnew, type = "probability")
head(predC45)
N <- nrow(predC45)
logloss<- -(sum(log(predC45)*train_testnew[,15:18]))/N
logloss
N
0.7*38000
0.3*38000
predC45
library(C50)
C50model1 <- C5.0(Class ~ ., data = traindatanew)
C50model1 <- C5.0(Class ~ ., data = train_trainnew)
predC50 <- predict.C5.0(C50model1, newdata = train_testnew, type = "prob")
head(predC50)
N <- nrow(predC50)
logloss<- -(sum(log(predC50)*train_testnew[,15:18]))/N
logloss
model8 <- mlogit(Choice ~ CC+GN+NS+BU+LD+BZ+FC+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price-1, data = train_train) #without FA, RP, FP
predC45 <- predict(modelC45, newdata = train_testnew, type = "class")
table(train_testnew[,15:18]), predC45)
head(predC45)
train_testnew[,15:18]
predC45 <- predict(modelC45, newdata = train_testnew, type = "probabilitiy")
predC45 <- predict(modelC45, newdata = train_testnew, type = "probability")
N <- nrow(predC45)
logloss<- -(sum(log(predC45)*train_testnew[,16:19]))/N
logloss
predC50 <- predict.C5.0(C50model1, newdata = train_testnew, type = "prob")
N <- nrow(predC50)
logloss<- -(sum(log(predC50)*train_testnew[,16:19]))/N
logloss
head(predC45)
head(predC45, 100)
rf <- randomForest(Class ~ ., data = train_trainnew, importance = TRUE)
library(randomForest)
rf <- randomForest(Class ~ ., data = train_trainnew, importance = TRUE)
head(predrf)
predrf <- predict(rf, newdata = testdatanew, type = "prob")
predrf <- predict(rf, newdata = train_testnew, type = "prob")
head(predrf)
WOW(Bagging)
Bagging(Class ~ ., data = train_trainnew)
modelBag1 <- Bagging(Class ~ ., data = train_trainnew)
predBag <- predict(modelBag1, newdata = train_testnew)
head(predBag)
predBag <- predict(modelBag1, newdata = train_testnew, type = "probbb")
predBag <- predict(modelBag1, newdata = train_testnew, type = "probability")
head(predBag)
N <- nrow(predBag)
logloss<- -(sum(log(predBag)*train_testnew[,16:19]))/N
logloss
predBag
modelBag1 <- Bagging(Class ~ ., data = train_trainnew, control = Weka_control(A = TRUE))
WOW(Bagging)
modelBag1 <- Bagging(Class ~ ., data = train_trainnew, control = Weka_control( W = "weka.classifiers.trees.J48"))
predBag <- predict(modelBag1, newdata = train_testnew, type = "probability")
head(predBag)
N <- nrow(predBag)
logloss<- -(sum(log(predBag)*train_testnew[,16:19]))/N
logloss
logloss<- -(sum(log(predBag)*train_testnew[,16:19], na.rm = TRUE))/N
logloss
predBag <- predict(modelBag1, newdata = train_testnew, type = "prob")
predBag <- predict(modelBag1, newdata = train_testnew, type = "aprob")
predBag <- predict(modelBag1, newdata = train_testnew)
head(predBag)
predBag <- predict(modelBag1, newdata = train_testnew, type = "probability")
log(0.00001)
?log
logloss<- -(sum(log(predBag, na.rm = TRUE)*train_testnew[,16:19]))/N
head(predC45, 100)
N <- nrow(predC45)
logloss<- -(sum(log(predC45)*train_testnew[,16:19]))/N
logloss
modelBag1 <- Stacking(Class ~ ., data = train_trainnew, control = Weka_control( W = "weka.classifiers.trees.J48"))
predBag <- predict(modelBag1, newdata = train_testnew, type = "probability")
head(predBag)
N <- nrow(predBag)
logloss<- -(sum(log(predBag)*train_testnew[,16:19]))/N
logloss
head(predBag,20)
modelBag1 <- AdaBoostM1(Class ~ ., data = train_trainnew, control = Weka_control( W = "weka.classifiers.trees.J48"))
predBag <- predict(modelBag1, newdata = train_testnew, type = "probability")
head(predBag,20)
N <- nrow(predBag)
logloss<- -(sum(log(predBag)*train_testnew[,16:19]))/N
logloss
predmlogitX <- predict(model8, newdata = train_test)
dim(predmlogitX)
N <- nrow(train_test)
N
dim(predmlogitX)
>>>>>>> 5f9e04148ccf1ba53a4fc3e1342511022067ee88
