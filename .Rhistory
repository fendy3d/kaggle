summary(model1)
predtest1<-predict(model1, newdata=test)
sse1<-sum((test$Apps-predtest1)^2)
sse1
sse1<-mean((test$Apps-predtest1)^2)
sse1
mse
mse<-mean((test$Apps-predtest1)^2)
mse
ncol(College)
str(college)
str(College)
library(leaps)
model2.fwd <- regsubsets(Apps ~ ., data = College, nvmax = ncol(College) - 1, method = "forward")
summary(model2.fwd)
ncol(Hitters)
val.errors<-rep(NA,17)
for(i in 1:17){
coefi<-coef(model2.fwd,id=i)
pred<- test.mat[,names(coefi)]%*%coefi   # there is no predict() method for regsubsets()
val.errors[i]<-mean((test$Apps-pred)^2)
}
val.errors
test.mat<-model.matrix(Apps ~ ., data=test)
val.errors<-rep(NA,17)
for(i in 1:17){
coefi<-coef(model2.fwd,id=i)
pred<- test.mat[,names(coefi)]%*%coefi   # there is no predict() method for regsubsets()
val.errors[i]<-mean((test$Apps-pred)^2)
}
val.errors
which.min(val.errors)
var.length <- ncol(College) - 1
test.mat<-model.matrix(Apps ~ ., data=test)
var.length <- ncol(College) - 1
var.length
k<-10
set.seed(1)
folds <- sample(1:k, nrow(College), replace=TRUE)
cv.errors<-matrix(NA, nrow=k, ncol=var.length, dimnames=list(NULL, paste(1:var.length)))
cv.errors
predict.regsubsets <- function(object,newdata,id,...){
form <- as.formula(object$call[[2]])
mat <- model.matrix(form,newdata)
coefi <- coef(object,id=id)
return(mat[,names(coefi)]%*%coefi)
}
for (j in 1:k){
fwd.fit <- regsubsets(Apps ~. ,data = College[folds!=j,], nvmax = var.length, method = "Forward")
for (i in 1:var.length){
pred <- predict.regsubsets(fwd.fit, College[folds==j,], id = i)
cv.errors[j,i] <- mean((College$Apps[folds==j]-pred)^2)
}
}
library(leaps)
for (j in 1:k){
fwd.fit <- regsubsets(Apps ~. ,data = College[folds!=j,], nvmax = var.length, method = "forward")
for (i in 1:var.length){
pred <- predict.regsubsets(fwd.fit, College[folds==j,], id = i)
cv.errors[j,i] <- mean((College$Apps[folds==j]-pred)^2)
}
}
cv.errors
?apply
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors
mean(cv.errors[,1])
which.min(mean.cv.errors)
?"glmnet"
train.mat<-model.matrix(Apps ~ ., data = train)
var.length <- ncol(College) - 1
k<-10
set.seed(1)
folds <- sample(1:k, nrow(College), replace = TRUE)
cv.errors <- matrix(NA, nrow = k, ncol = var.length, dimnames = list(NULL, paste(1:var.length)))
cv.errors #creating an empty 10 x 17 matrix with ncol = 17 for each fold of the model
predict.regsubsets <- function(object,newdata,id,...){
form <- as.formula(object$call[[2]])
mat <- model.matrix(form,newdata)
coefi <- coef(object,id=id)
}
return(mat[,names(coefi)]%*%coefi)
for (j in 1:k){
fwd.fit <- regsubsets(Apps ~. ,data = College[folds!=j,], nvmax = var.length, method = "forward")
for (i in 1:var.length){
pred <- predict.regsubsets(fwd.fit, College[folds==j,], id = i)
cv.errors[j,i] <- mean((College$Apps[folds==j]-pred)^2)
}
}
cv.errors # This is the answer to the question
mean.cv.errors <- apply(cv.errors, 2, mean) #2 here means apply the mean on the data in a column
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors,type='b')
which.min(mean.cv.errors)
train.mat <- model.matrix(Apps ~ ., data = College.train)
train.mat <- model.matrix(Apps ~ ., data = train)
test.mat <- model.matrix(Apps ~ ., data = test)
grid<-10^seq(4,-2, length=100)
grid
ridge.model <- glmnet(train.mat, train$Apps, alpha=0, lambda=grid)
?"glmnet"
cv.ridge <- cv.glmnet(train.mat, train$Apps, alpha = 0, lambda = grid)
cv.ridge
bestlambda <- cv.ridge$lambda.min
bestlambda
?predict
pred.ridge <- predict(ridge.model, s = bestlambda, newx = test.mat)
mean((pred.ridge - test$Apps)^2)
ridge.model <- glmnet(train.mat, train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
cv.ridge <- cv.glmnet(train.mat, train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
bestlambda <- cv.ridge$lambda.min
bestlambda
pred.ridge <- predict(ridge.model, s = bestlambda, newx = test.mat)
mean((pred.ridge - test$Apps)^2)
test <- -t
test <- -t
train2 <- College[test,]
train2
testt <- College[!t,]
testt
t <- sample(c(T,F), nrow(College), replace = TRUE) #sample(x, size, replace = TRUE) x: a vector of elements from which to sample, size: is the number of times to sample, replace: replace elements after sampling
train <- College[t,]
test <- College[!t,]
model1 <- lm(Apps ~ ., data = train)
predtest1<-predict(model1, newdata=test)
mse<-mean((test$Apps-predtest1)^2)
library(leaps)
train.mat <- model.matrix(Apps ~ ., data = train)
var.length <- ncol(College) - 1
k<-10
set.seed(1)
folds <- sample(1:k, nrow(College), replace = TRUE)
cv.errors <- matrix(NA, nrow = k, ncol = var.length, dimnames = list(NULL, paste(1:var.length)))
cv.errors #creating an empty 10 x 17 matrix with ncol = 17 for each fold of the model
predict.regsubsets <- function(object,newdata,id,...){
form <- as.formula(object$call[[2]])
mat <- model.matrix(form,newdata)
coefi <- coef(object,id=id)
return(mat[,names(coefi)]%*%coefi)
}
for (j in 1:k){
fwd.fit <- regsubsets(Apps ~. ,data = College[folds!=j,], nvmax = var.length, method = "forward")
for (i in 1:var.length){
pred <- predict.regsubsets(fwd.fit, College[folds==j,], id = i)
cv.errors[j,i] <- mean((College$Apps[folds==j]-pred)^2)
}
}
cv.errors # This is the answer to the question
mean.cv.errors <- apply(cv.errors, 2, mean) #2 here means apply the mean on the data in a column
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors,type='b')
which.min(mean.cv.errors) #gives 1
train.mat <- model.matrix(Apps ~ ., data = train) # creating the "x" with the training data set
test.mat <- model.matrix(Apps ~ ., data = test) # creating the "x" with the test data set
grid <- 10^seq(4,-2, length=100) # creating a lot of lambda values so that with each value, it changes the coefficients of the model
ridge.model <- glmnet(train.mat, train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
cv.ridge <- cv.glmnet(train.mat, train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
bestlambda <- cv.ridge$lambda.min
bestlambda
pred.ridge <- predict(ridge.model, s = bestlambda, newx = test.mat)
mean((pred.ridge - test$Apps)^2)
set.seed(1)
x
nrow(x)
dim(College)
nrow(train.mat)
x<-model.matrix(Apps ~ ., data = College)[,-1]
nrow(x)
College.train <- College[train, ]
College.test <- College[test, ]
train <- sample(1:dim(College)[1], dim(College)[1]/2)
test<- -train
College.train <- College[train, ]
College.test <- College[test, ]
train.mat <- model.matrix(Apps ~ ., data = College.train) # creating the "x" with the training data set
test.mat <- model.matrix(Apps ~ ., data = College.test) # creating the "x" with the test data set
grid <- 10^seq(4,-2, length=100) # creating a lot of lambda values so that with each value, it changes the coefficients of the model
ridge.model <- glmnet(train.mat, train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
cv.ridge <- cv.glmnet(train.mat, train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
ridge.model <- glmnet(train.mat, College.train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
cv.ridge <- cv.glmnet(train.mat, College.train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
bestlambda <- cv.ridge$lambda.min
bestlambda
pred.ridge <- predict(ridge.model, s = bestlambda, newx = test.mat)
mean((pred.ridge - test$Apps)^2)
mean((pred.ridge - College.test$Apps)^2)
set.seed(11)
train <- sample(1:dim(College)[1], dim(College)[1]/2)
test<- -train
College.train <- College[train, ]
College.test <- College[test, ]
train.mat <- model.matrix(Apps ~ ., data = College.train) # creating the "x" with the training data set
test.mat <- model.matrix(Apps ~ ., data = College.test) # creating the "x" with the test data set
grid <- 10^seq(4,-2, length=100) # creating a lot of lambda values so that with each value, it changes the coefficients of the model
ridge.model <- glmnet(train.mat, College.train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
cv.ridge <- cv.glmnet(train.mat, College.train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
bestlambda <- cv.ridge$lambda.min
bestlambda
pred.ridge <- predict(ridge.model, s = bestlambda, newx = test.mat)
mean((pred.ridge - College.test$Apps)^2)
set.seed(1)
train <- sample(1:dim(College)[1], dim(College)[1]/2)
test<- -train
College.train <- College[train, ]
College.test <- College[test, ]
train.mat <- model.matrix(Apps ~ ., data = College.train) # creating the "x" with the training data set
test.mat <- model.matrix(Apps ~ ., data = College.test) # creating the "x" with the test data set
grid <- 10^seq(4,-2, length=100) # creating a lot of lambda values so that with each value, it changes the coefficients of the model
ridge.model <- glmnet(train.mat, College.train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
cv.ridge <- cv.glmnet(train.mat, College.train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
bestlambda
bestlambda <- cv.ridge$lambda.min
pred.ridge <- predict(ridge.model, s = bestlambda, newx = test.mat)
mean((pred.ridge - College.test$Apps)^2)
library(glmnet)
set.seed(11)
train <- sample(1:dim(College)[1], dim(College)[1]/2)
test<- -train
College.train <- College[train, ]
College.test <- College[test, ]
train.mat <- model.matrix(Apps ~ ., data = College.train) # creating the "x" with the training data set
test.mat <- model.matrix(Apps ~ ., data = College.test) # creating the "x" with the test data set
grid <- 10^seq(4,-2, length=100) # creating a lot of lambda values so that with each value, it changes the coefficients of the model
lasso.model <- glmnet(train.mat, College.train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
cv.lasso <- cv.glmnet(train.mat, College.train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
bestlambda <- cv.lasso$lambda.min
bestlambda
pred.lasso <- predict(lasso.model, s = bestlambda, newx = test.mat)
mean((pred.lasso - College.test$Apps)^2)
lasso.model <- glmnet(train.mat, College.train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
cv.lasso <- cv.glmnet(train.mat, College.train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
bestlambda <- cv.lasso$lambda.min
bestlambda
pred.lasso <- predict(lasso.model, s = bestlambda, newx = test.mat)
mean((pred.lasso - College.test$Apps)^2)
```
0.7*0.7*0.6
svm1 <- svm(Class ~ ., data = traindatanew) # "cross" 10 cross-validations
library(e1071)
svm1 <- svm(Class ~ ., data = traindatanew) # "cross" 10 cross-validations
convertincometointeger <- function(train){
train$income <- ifelse(train$income == "Under $29,999", 15000, ifelse(train$income == "$30,000 to $39,999", 35000, ifelse(train$income =="$40,000 to $49,999", 45000, ifelse(train$income =="$50,000 to $59,999", 55000, ifelse(train$income =="$60,000 to $69,999", 65000, ifelse(train$income =="$70,000 to $79,999", 75000, ifelse(train$income =="$80,000 to $89,999", 85000, ifelse(train$income =="$90,000 to $99,999", 95000, ifelse(train$income =="$100,000 to $109,999", 105000, ifelse(train$income == "$110,000 to $119,999", 115000, ifelse(train$income == "$120,000 to $129,999", 125000, ifelse(train$income == "$130,000 to $139,999", 135000, ifelse(train$income == "$140,000 to $149,999", 145000, ifelse(train$income == "$150,000 to $159,999", 155000, ifelse(train$income == "$160,000 to $169,999", 165000, ifelse(train$income == "$170,000 to $179,999", 175000, ifelse(train$income == "$190,000 to $199,999", 195000, ifelse(train$income == "$200,000 to $209,999", 205000, ifelse(train$income == "$220,000 to $229,999", 225000, ifelse(train$income == "$250,000 to $259,999", 255000, ifelse(train$income == "$300,000 & Over", 320000, NA)))))))))))))))))))))
return (train)
}
convertpricetointeger <- function(train){
train <- ifelse(train == 1, 500, ifelse(train == 2, 1000, ifelse(train ==3, 1500, ifelse(train == 4, 2000, ifelse(train == 5, 2500, ifelse(train == 6, 3000, ifelse(train == 7, 4000, ifelse(train == 8, 5000, ifelse(train == 9, 7500, ifelse(train == 10, 10000, ifelse(train == 11, 12000, 1)))))))))))
return (train)
}
# manage inconsistencies in income (only for training)
convertincome_train <- function(train){
N <- nrow(train)
for (i in 1:N){
if (train[i,93] == "$290,000 to $299,999"){
train[i,93] <- as.character("$300,000 & Over")
}
}
return (train)
}
# manage inconsistencies in income (only for test)
convertincome_test <- function(test){
N <- nrow(test)
for (i in 1:N){
if (as.character(test[i,93]) == "$180,000 to $189,999"){
test[i,93] = as.character("$170,000 to $179,999")
} else if (as.character(test[i,93]) == "$230,000 to $239,999"){
test[i,93] = as.character("$220,000 to $229,999")
} else if (as.character(test[i,93]) == "$270,000 to $279,999"){
test[i,93] = as.character("$250,000 to $259,999")
}
return (test)
}
}
# factorise the variables (segment, year .. ppark)
factorise <- function(train){
train$segment <- factor(train$segment)
train$year <- factor(train$year)
train$miles <- factor(train$miles)
train$night <- factor(train$night)
train$gender <- factor(train$gender)
train$age <- factor(train$age)
train$educ <- factor(train$educ)
train$region <- factor(train$region)
train$Urb <- factor(train$Urb)
train$ppark <- factor(train$ppark)
return (train)
}
train <- read.csv("train.csv", stringsAsFactors = FALSE)
train <- convertincometointeger(convertincome_train(train)) # converts '$290,000 to $299,999' to '$300,000 and over', then take the middle value.
train <- factorise(train) # factorise the variables (segment, year .. ppark)
train$Price1 <- convertpricetointeger(train$Price1)
train$Price2 <- convertpricetointeger(train$Price2)
train$Price3 <- convertpricetointeger(train$Price3)
train$Price4 <- convertpricetointeger(train$Price4)
setwd("C:/Users/Guanhua/Dropbox/SUTD/LECTURE NOTES/Term 8/Analytics wd/Kaggle/kaggle")
train <- read.csv("train.csv", stringsAsFactors = FALSE)
train <- convertincometointeger(convertincome_train(train)) # converts '$290,000 to $299,999' to '$300,000 and over', then take the middle value.
train <- factorise(train) # factorise the variables (segment, year .. ppark)
train$Price1 <- convertpricetointeger(train$Price1)
train$Price2 <- convertpricetointeger(train$Price2)
train$Price3 <- convertpricetointeger(train$Price3)
train$Price4 <- convertpricetointeger(train$Price4)
library(mlogit)
traindata <- mlogit.data(train, shape = "wide", choice = "Choice", varying = c(4:83), sep = "", alt.levels = c("Ch1","Ch2","Ch3","Ch4"), id.var = "Case")
View(traindata)
traindata$Class <- 0
for (i in 1:38000){
for (j in 15:18){
if (traindata[i,j] == 1){
traindata[i,]$Class <- colnames(traindata[j])
}
}
}
write.csv(traindata, "traindata.csv")
testdata$Class <- 0
test <- read.csv("test.csv", stringsAsFactors = FALSE)
test[is.na(test)] <- 0 # adds 0 to all missing values
test <- convertincometointeger(convertincome_test(test))
test <- factorise(test)
test$Price1 <- convertpricetointeger(test$Price1)
test$Price2 <- convertpricetointeger(test$Price2)
test$Price3 <- convertpricetointeger(test$Price3)
test$Price4 <- convertpricetointeger(test$Price4)
testdata <- mlogit.data(test, shape = "wide", choice = "Choice", varying = c(4:83), sep = "", alt.levels = c("Ch1","Ch2","Ch3","Ch4"), id.var = "Case")
testdata$Class <- 0
traindata <- read.csv("traindata.csv")
library(caTools)
train_train <- subset(traindata, spl == TRUE)
spl <- sample.split(traindata$Class, SplitRatio = 0.7)
train_train <- subset(traindata, spl == TRUE)
test_train <- subset(traindata, spl == FALSE)
View(train_train)
train_trainnew <- subset(train_train, select = - c(1,2,3,15,16,17,18,19,20,41)) # remove ch1,ch2,ch3,ch4,chid,alt columns
train_trainnew$Class <- as.factor(traindatanew$Class)
train_trainnew$Class <- as.factor(train_trainnew$Class)
testdata <- subset(test_rf, select = - c(1,2,3,15,16,17,18,19,20,41)) # remove ch1,ch2,ch3,ch4,chid,alt columns
train_test <- subset(traindata, spl == FALSE)
train_testnew <- subset(train_test, select = - c(1,2,3,15,16,17,18,19,20,41)) # remove ch1,ch2,ch3,ch4,chid,alt columns
svm1 <- svm(Class ~ ., data = train_trainnew) # "cross" 10 cross-validations
predsvm1 <- predict(svm1, newdata = train_testnew, type = "hhh")
head(predsvm1)
predsvm1 <- predict(svm1, newdata = train_testnew, type = "probability")
head(predsvm1)
?svm
summary(smv1)
svm1
summary(svm1)
svm1 <- svm(Class ~ ., data = train_trainnew, probability = TRUE)
predsvm1 <- predict(svm1, newdata = train_testnew)
head(predsvm1)
predsvm1 <- predict(svm1, newdata = train_testnew, probability = TRUE)
head(predsvm1)
predsvm1 <- predict(svm1, newdata = train_testnew, decision.values = TRUE, probability = TRUE)
head(predsvm1)
library(RWeka)
library(FSelector)
information.gain(Class ~ ., data = train_trainnew)
str(train_trainnew)
View(traindata)
traindata[,1]
traindata[,2]
names(traindata[,2])
colnames(traindata[,2])
traindata[41,]
traindata[,41]
train_trainnew <- subset(train_train, select = - c(2,3,4,16,17,18,19,20,21,42)) # remove ch1,ch2,ch3,ch4,chid,alt columns
spl <- sample.split(traindata$Class, SplitRatio = 0.7)
train_train <- subset(traindata, spl == TRUE)
train_test <- subset(traindata, spl == FALSE)
train_trainnew <- subset(train_train, select = - c(2,3,4,16,17,18,19,20,21,42)) # remove ch1,ch2,ch3,ch4,chid,alt columns
train_trainnew$Class <- as.factor(train_trainnew$Class)
train_testnew <- subset(train_test, select = - c(2,3,4,16,17,18,19,20,21,42)) # remove ch1,ch2,ch3,ch4,chid,alt columns
View(train_trainnew)
str(train_trainnew)
train_trainnew$year <- factor(train_trainnew)
spl <- sample.split(traindata$Class, SplitRatio = 0.7)
train_train <- subset(traindata, spl == TRUE)
train_test <- subset(traindata, spl == FALSE)
traindata <- read.csv("traindata.csv")
traindata$Class <- as.factor(train_trainnew$Class)
traindata <- read.csv("traindata.csv")
traindata$Class <- factor(traindata$Class)
traindata$year <- factor(traindata)
traindata$year <- factor(traindata$year)
library(caTools)
spl <- sample.split(traindata$Class, SplitRatio = 0.7)
train_train <- subset(traindata, spl == TRUE)
train_test <- subset(traindata, spl == FALSE)
train_trainnew <- subset(train_train, select = - c(2,3,4,16,17,18,19,20,21,42)) # remove ch1,ch2,ch3,ch4,chid,alt columns
train_testnew <- subset(train_test, select = - c(2,3,4,16,17,18,19,20,21,42)) # remove ch1,ch2,ch3,ch4,chid,alt columns
test <- read.csv("test.csv", stringsAsFactors = FALSE)
test[is.na(test)] <- 0 # adds 0 to all missing values
test <- convertincometointeger(convertincome_test(test))
test <- factorise(test)
test$Price1 <- convertpricetointeger(test$Price1)
test$Price2 <- convertpricetointeger(test$Price2)
test$Price3 <- convertpricetointeger(test$Price3)
test$Price4 <- convertpricetointeger(test$Price4)
testdata <- mlogit.data(test, shape = "wide", choice = "Choice", varying = c(4:83), sep = "", alt.levels = c("Ch1","Ch2","Ch3","Ch4"), id.var = "Case")
library(RWeka)
library(FSelector)
information.gain(Class ~ ., data = train_trainnew)
traindata <- read.csv("traindata.csv")
traindata$Class <- factor(traindata$Class)
traindata$year <- factor(traindata$year)
View(traindata)
library(caTools)
spl <- sample.split(traindata$Class, SplitRatio = 0.7)
train_train <- subset(traindata, spl == TRUE)
train_test <- subset(traindata, spl == FALSE)
train_trainnew <- subset(train_train, select = - c(1,2,3,4,16,17,18,19,20,21,42)) # remove ch1,ch2,ch3,ch4,chid,alt columns
train_testnew <- subset(train_test, select = - c(1,2,3,4,16,17,18,19,20,21,42)) # remove ch1,ch2,ch3,ch4,chid,alt columns
information.gain(Class ~ ., data = train_trainnew)
source('~/.active-rstudio-document', echo=TRUE)
information.gain(Class ~ ., data = train_trainnew)
modelC45 <- J48(Class ~ ., data = train_trainnew)
WOW(J48)
modelC45 <- J48(Class ~ ., data = train_trainnew, control = Weka_control(A = TRUE))
predC45 <- predict(modelC45, newdata = train_testnew, type = "haha")
predC45 <- predict(modelC45, newdata = train_testnew, type = "probability")
head(predC45)
N <- nrow(predC45)
logloss<- -(sum(log(predC45)*train_testnew[,15:18]))/N
logloss
N
0.7*38000
0.3*38000
predC45
library(C50)
C50model1 <- C5.0(Class ~ ., data = traindatanew)
C50model1 <- C5.0(Class ~ ., data = train_trainnew)
predC50 <- predict.C5.0(C50model1, newdata = train_testnew, type = "prob")
head(predC50)
N <- nrow(predC50)
logloss<- -(sum(log(predC50)*train_testnew[,15:18]))/N
logloss
model8 <- mlogit(Choice ~ CC+GN+NS+BU+LD+BZ+FC+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price-1, data = train_train) #without FA, RP, FP
predC45 <- predict(modelC45, newdata = train_testnew, type = "class")
table(train_testnew[,15:18]), predC45)
head(predC45)
train_testnew[,15:18]
predC45 <- predict(modelC45, newdata = train_testnew, type = "probabilitiy")
predC45 <- predict(modelC45, newdata = train_testnew, type = "probability")
N <- nrow(predC45)
logloss<- -(sum(log(predC45)*train_testnew[,16:19]))/N
logloss
predC50 <- predict.C5.0(C50model1, newdata = train_testnew, type = "prob")
N <- nrow(predC50)
logloss<- -(sum(log(predC50)*train_testnew[,16:19]))/N
logloss
head(predC45)
head(predC45, 100)
rf <- randomForest(Class ~ ., data = train_trainnew, importance = TRUE)
library(randomForest)
rf <- randomForest(Class ~ ., data = train_trainnew, importance = TRUE)
head(predrf)
predrf <- predict(rf, newdata = testdatanew, type = "prob")
predrf <- predict(rf, newdata = train_testnew, type = "prob")
head(predrf)
WOW(Bagging)
Bagging(Class ~ ., data = train_trainnew)
modelBag1 <- Bagging(Class ~ ., data = train_trainnew)
predBag <- predict(modelBag1, newdata = train_testnew)
head(predBag)
predBag <- predict(modelBag1, newdata = train_testnew, type = "probbb")
predBag <- predict(modelBag1, newdata = train_testnew, type = "probability")
head(predBag)
N <- nrow(predBag)
logloss<- -(sum(log(predBag)*train_testnew[,16:19]))/N
logloss
predBag
modelBag1 <- Bagging(Class ~ ., data = train_trainnew, control = Weka_control(A = TRUE))
WOW(Bagging)
modelBag1 <- Bagging(Class ~ ., data = train_trainnew, control = Weka_control( W = "weka.classifiers.trees.J48"))
predBag <- predict(modelBag1, newdata = train_testnew, type = "probability")
head(predBag)
N <- nrow(predBag)
logloss<- -(sum(log(predBag)*train_testnew[,16:19]))/N
logloss
logloss<- -(sum(log(predBag)*train_testnew[,16:19], na.rm = TRUE))/N
logloss
predBag <- predict(modelBag1, newdata = train_testnew, type = "prob")
predBag <- predict(modelBag1, newdata = train_testnew, type = "aprob")
predBag <- predict(modelBag1, newdata = train_testnew)
head(predBag)
predBag <- predict(modelBag1, newdata = train_testnew, type = "probability")
log(0.00001)
?log
logloss<- -(sum(log(predBag, na.rm = TRUE)*train_testnew[,16:19]))/N
head(predC45, 100)
N <- nrow(predC45)
logloss<- -(sum(log(predC45)*train_testnew[,16:19]))/N
logloss
modelBag1 <- Stacking(Class ~ ., data = train_trainnew, control = Weka_control( W = "weka.classifiers.trees.J48"))
predBag <- predict(modelBag1, newdata = train_testnew, type = "probability")
head(predBag)
N <- nrow(predBag)
logloss<- -(sum(log(predBag)*train_testnew[,16:19]))/N
logloss
head(predBag,20)
modelBag1 <- AdaBoostM1(Class ~ ., data = train_trainnew, control = Weka_control( W = "weka.classifiers.trees.J48"))
predBag <- predict(modelBag1, newdata = train_testnew, type = "probability")
head(predBag,20)
N <- nrow(predBag)
logloss<- -(sum(log(predBag)*train_testnew[,16:19]))/N
logloss
predmlogitX <- predict(model8, newdata = train_test)
dim(predmlogitX)
N <- nrow(train_test)
N
dim(predmlogitX)
